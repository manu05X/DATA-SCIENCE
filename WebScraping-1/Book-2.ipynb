{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES\n",
    "http://books.toscrape.com/catalogue/page-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\\n<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\\n<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\\n<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\\n    <head>\\n        <title>\\n    All products | Books to Scrape - Sandbox\\n</title>\\n\\n        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" />\\n        <meta name=\"created\" content=\"24th Jun 2016 09:29\" />\\n        <meta name=\"description\" content=\"\" />\\n        <meta name=\"viewport\" content=\"width=device-width\" />\\n        <meta name=\"robots\" content=\"NOARCHIVE,NOCACHE\" />\\n\\n        <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\\n        <!--[if lt IE 9]>\\n        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\\n        <![endif]-->\\n\\n        \\n            <link rel=\"shortcut icon\" href=\"static/oscar/favicon.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_url = \"http://books.toscrape.com/index.html\"\n",
    "\n",
    "import requests\n",
    "result = requests.get(main_url)\n",
    "\n",
    "result.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" lang=\"en-us\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <title>\n",
      "   All products | Books to Scrape - Sandbox\n",
      "  </title>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "  <meta content=\"24th Jun 2016 09:29\" name=\"created\"/>\n",
      "  <meta content=\"\" name=\"description\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <meta content=\"NOARCHIVE,NOCACHE\" name=\"robots\"/>\n",
      "  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\n",
      "  <!--[if lt IE 9]>\n",
      "        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\n",
      "        <![endif]-->\n",
      "  <link href=\"static/oscar/favicon.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"static/oscar/css/styles.css\" rel=\"stylesheet\" type=\"tex\n"
     ]
    }
   ],
   "source": [
    "#The function prettify() makes the HTML more readable. However we will not use this directly to explore where the relevant data is.\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s define a function to request and parse a HTML web page as we will need this a lot\n",
    "def getAndParseURL(url):\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find book URLs on the main page\n",
    "Now let’s start to dive deeper into the subject. In order to get the book data, we need to be able to access their product page. The first step consist in finding the URL of every book product page.\n",
    "In your browser, go onto the website main page, right-click on the name of a product and click on inspect. This will show you the HTML part of the web page corresponding to this element. Congratulations, you have found the first book link!\n",
    "Note the structure of the HTML code:\n",
    "\n",
    "Inspecting HTML code\n",
    "You can try this with every other product on the page: the structure is always the same. The link of the product corresponds to the ‘href’ attribute of the ‘a’ tag. This one belongs to an ‘article’ tag with the a class value ‘product_pod’. This seems to be a reliable source to spot product URLs.\n",
    "BeautifulSoup enables us to find those special ‘article’ tags. We can wall the find() function in order to find the first occurence of this tag in the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<article class=\"product_pod\">\n",
       "<div class=\"image_container\">\n",
       "<a href=\"catalogue/a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>\n",
       "</div>\n",
       "<p class=\"star-rating Three\">\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "<i class=\"icon-star\"></i>\n",
       "</p>\n",
       "<h3><a href=\"catalogue/a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\n",
       "<div class=\"product_price\">\n",
       "<p class=\"price_color\">Â£51.77</p>\n",
       "<p class=\"instock availability\">\n",
       "<i class=\"icon-ok\"></i>\n",
       "    \n",
       "        In stock\n",
       "    \n",
       "</p>\n",
       "<form>\n",
       "<button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">Add to basket</button>\n",
       "</form>\n",
       "</div>\n",
       "</article>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"article\", class_ = \"product_pod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"catalogue/a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We still have too much information.Let’s dive deeper in the tree by adding the other child tags:\n",
    "soup.find(\"article\", class_ = \"product_pod\").div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catalogue/a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Much better! But we only need the URL contained in the ‘href’ value.\n",
    "#We can get this by adding .get(“href”) to the previous instruction:\n",
    "soup.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we managed to get our first product URL with BeautifulSoup.\n",
    "Now let’s gather all the products URLs on the main web page at once using the findAll() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 fetched products URLs\n",
      "One example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'catalogue/a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_page_products_urls = [x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "print(str(len(main_page_products_urls)) + \" fetched products URLs\")\n",
    "print(\"One example:\")\n",
    "main_page_products_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is very handy for finding all the values at once, but you have to check that all the information collected is relevant. Sometimes one same tag can contain completely different data. That is why it is important to be as specific as possible when choosing the tags. Here we decided to rely on the tag ‘article’ with the ‘product_pod’ class because this seems to be a very specific tag and it is unlikely that we can find data other than product data in it.\n",
    "The previous URLs correspond to their relative path from the main page. In order to make them complete, we just need to add before them the URL of the main page: http://books.toscrape.com/index.html (after removing the index.html part).\n",
    "Now let’s use this to define a function to retrieve book links on any given page of the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getBooksURLs(url):\n",
    "    soup = getAndParseURL(url)\n",
    "    # remove the index.html part of the base url before returning the results\n",
    "    return([\"/\".join(url.split(\"/\")[:-1]) + \"/\" + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find book categories URLs on the main page\n",
    "Now let’s try retrieving the URLs corresponding the different product categories:\n",
    "\n",
    "Inspecting HTML code\n",
    "By inspecting, we can see that they follow the same URL pattern: ‘catalogue/category/books’.\n",
    "We can tell BeautifulSoup to match the URLs that contain this pattern in order to retrieve easily the categories URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched categories URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/index.htmlcatalogue/category/books/travel_2/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/mystery_3/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/historical-fiction_4/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/sequential-art_5/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/classics_6/index.html']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "categories_urls = [main_url + x.get('href') for x in soup.find_all(\"a\", href=re.compile(\"catalogue/category/books\"))]\n",
    "categories_urls = categories_urls[1:] # we remove the first one because it corresponds to all the books\n",
    "\n",
    "print(str(len(categories_urls)) + \" fetched categories URLs\")\n",
    "print(\"Some examples:\")\n",
    "categories_urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape all books data\n",
    "For the last part of this tutorial, we will finally tackle our main objective: gather data about all the books of the website.\n",
    "We know how to get the links of the books within a given page. If all the books were displayed on a same page this would be easy. However this situation is unlikely as it is not very user friendly to display all the catalog to the user on the same page.\n",
    "Usually products are displayed on multiple pages or on one page but through scrolling. We can see here at the bottom of the main page that there are 50 products pages and a button ‘next’ to access to the next product page.\n",
    "\n",
    "End of the main page\n",
    "On the next pages there is also a ‘previous’ button to come back to the last product page.\n",
    "\n",
    "End of the second page\n",
    "Get all pages URLs\n",
    "In order to fetch all the products URLs, we need to be able to get through all the pages. To do so, we can go iteratively through all the ‘next’ buttons.\n",
    "\n",
    "Inspecting HTML code\n",
    "The ‘next’ button contains the pattern ‘page’. We can use this to retrieve the URLs of the next pages. But let’s be careful: the ‘previous’ button also contains this pattern!\n",
    "If we have two results when matching with ‘page’, we should take the second one as it will correspond to the next page. For the first and the last pages we will have only one result because we will have either the ‘next’ button or the ‘previous’ button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/index.html',\n",
       " 'http://books.toscrape.com/catalogue/page-2.html',\n",
       " 'http://books.toscrape.com/catalogue/page-3.html',\n",
       " 'http://books.toscrape.com/catalogue/page-4.html',\n",
       " 'http://books.toscrape.com/catalogue/page-5.html']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store all the results into a list\n",
    "pages_urls = [main_url]\n",
    "\n",
    "soup = getAndParseURL(pages_urls[0])\n",
    "\n",
    "# while we get two matches, this means that the webpage contains a 'previous' and a 'next' button\n",
    "# if there is only one button, this means that we are either on the first page or on the last page\n",
    "# we stop when we get to the last page\n",
    "\n",
    "while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(pages_urls) == 1:\n",
    "    \n",
    "    # get the new complete url by adding the fetched URL to the base URL (and removing the .html part of the base URL)\n",
    "    new_url = \"/\".join(pages_urls[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "    \n",
    "    # add the URL to the list\n",
    "    pages_urls.append(new_url)\n",
    "    \n",
    "    # parse the next page\n",
    "    soup = getAndParseURL(new_url)\n",
    "    \n",
    "\n",
    "print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "pages_urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully managed to get the 50 pages URLs. What is interesting here is that the URL of those pages is highly predictable. We could have just created this list by incrementing ‘page-X.html’ until 50.\n",
    "This solution could work for this exact example but would not work anymore if the number of pages changed (e.g. if the website decided to print more products per pages, or if the catalog changed).\n",
    "One solution could be to increment the value until we get on a 404 page.\n",
    "Here we can see that trying to go to the 51th page effectively gets us a 404 error.\n",
    "Fortunately the result of a request has a very useful attribute that can show us the return status of the HTML request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code for page 50: 200\n",
      "status code for page 51: 404\n"
     ]
    }
   ],
   "source": [
    "result = requests.get(\"http://books.toscrape.com/catalogue/page-50.html\")\n",
    "print(\"status code for page 50: \" + str(result.status_code))\n",
    "\n",
    "result = requests.get(\"http://books.toscrape.com/catalogue/page-51.html\")\n",
    "print(\"status code for page 51: \" + str(result.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 200 code indicates that there is no error. The 404 code tells us that the page was not found.\n",
    "We can use this information to get all our pages URLs: we should iterate until we get a 404 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/catalogue/page-1.html',\n",
       " 'http://books.toscrape.com/catalogue/page-2.html',\n",
       " 'http://books.toscrape.com/catalogue/page-3.html',\n",
       " 'http://books.toscrape.com/catalogue/page-4.html',\n",
       " 'http://books.toscrape.com/catalogue/page-5.html']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_urls = []\n",
    "\n",
    "new_page = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "while requests.get(new_page).status_code == 200:\n",
    "    pages_urls.append(new_page)\n",
    "    new_page = pages_urls[-1].split(\"-\")[0] + \"-\" + str(int(pages_urls[-1].split(\"-\")[1].split(\".\")[0]) + 1) + \".html\"\n",
    "    \n",
    "\n",
    "print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "pages_urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all products URLs\n",
    "Now the next step consists in fetching all the products URLs for every page. This step is quite simple as we already have the list of all pages and the function to get products URLs from a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 fetched URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'http://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'http://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksURLs = []\n",
    "for page in pages_urls:\n",
    "    booksURLs.extend(getBooksURLs(page))\n",
    "    \n",
    "print(str(len(booksURLs)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "booksURLs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get product data\n",
    "The last step consist in scraping the data for each product\n",
    "We can easily retrieve a lot of information for every book:\n",
    "* book title\n",
    "* price\n",
    "* availability\n",
    "* image\n",
    "* category\n",
    "* rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>nb_in_stock</th>\n",
       "      <th>url_img</th>\n",
       "      <th>product_category</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>22</td>\n",
       "      <td>http://books.toscrape.com/catalogue/a-light-in...</td>\n",
       "      <td>poetry_23</td>\n",
       "      <td>Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/tipping-th...</td>\n",
       "      <td>historical-fiction_4</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/soumission...</td>\n",
       "      <td>fiction_10</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sharp-obje...</td>\n",
       "      <td>mystery_3</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sapiens-a-...</td>\n",
       "      <td>history_32</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  price nb_in_stock  \\\n",
       "0                   A Light in the Attic  51.77          22   \n",
       "1                     Tipping the Velvet  53.74          20   \n",
       "2                             Soumission  50.10          20   \n",
       "3                          Sharp Objects  47.82          20   \n",
       "4  Sapiens: A Brief History of Humankind  54.23          20   \n",
       "\n",
       "                                             url_img      product_category  \\\n",
       "0  http://books.toscrape.com/catalogue/a-light-in...             poetry_23   \n",
       "1  http://books.toscrape.com/catalogue/tipping-th...  historical-fiction_4   \n",
       "2  http://books.toscrape.com/catalogue/soumission...            fiction_10   \n",
       "3  http://books.toscrape.com/catalogue/sharp-obje...             mystery_3   \n",
       "4  http://books.toscrape.com/catalogue/sapiens-a-...            history_32   \n",
       "\n",
       "  rating  \n",
       "0  Three  \n",
       "1    One  \n",
       "2    One  \n",
       "3   Four  \n",
       "4   Five  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = []\n",
    "prices = []\n",
    "nb_in_stock = []\n",
    "img_urls = []\n",
    "categories = []\n",
    "ratings = []\n",
    "\n",
    "# scrape data for every book URL: this may take some time\n",
    "for url in booksURLs:\n",
    "    soup = getAndParseURL(url)\n",
    "    # product name\n",
    "    names.append(soup.find(\"div\", class_ = re.compile(\"product_main\")).h1.text)\n",
    "    # product price\n",
    "    prices.append(soup.find(\"p\", class_ = \"price_color\").text[2:]) # get rid of the pound sign\n",
    "    # number of available products\n",
    "    nb_in_stock.append(re.sub(\"[^0-9]\", \"\", soup.find(\"p\", class_ = \"instock availability\").text)) # get rid of non numerical characters\n",
    "    # image url\n",
    "    img_urls.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
    "    # product category\n",
    "    categories.append(soup.find(\"a\", href = re.compile(\"../category/books/\")).get(\"href\").split(\"/\")[3])\n",
    "    # ratings\n",
    "    ratings.append(soup.find(\"p\", class_ = re.compile(\"star-rating\")).get(\"class\")[1])\n",
    "    \n",
    "# add data into pandas df\n",
    "import pandas as pd\n",
    "\n",
    "scraped_data = pd.DataFrame({'name': names, 'price': prices, 'nb_in_stock': nb_in_stock, \"url_img\": img_urls, \"product_category\": categories, \"rating\": ratings})\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We got our data: our web scraping experiment is a success.\n",
    "\n",
    "\n",
    "\n",
    "Some data cleaning may be useful before using them:\n",
    "transform the ratings into numerical values\n",
    "remove the numbers in the product_category column\n",
    "Wrap up\n",
    "We have seen how to get through websites and gather data on each web page using automated web scrapers. One key thing in order to build efficient web scrapers is to understand the structure of the website on which you want to scrape the information. This means that you will probably have to maintain you scraper if you want it to remain useful after websites updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
